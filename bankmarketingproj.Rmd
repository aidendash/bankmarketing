---
title: "Yes or No? Classifying Term Deposit Subscriptions using Random Forest"
author: "Aiden"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Variable Description
-age

-job - type of job (categorical:'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')	
-marital - marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)	
-education - (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')	
-default - has credit in default?	
-balance - average yearly balance	
-housing - has housing loan?	
-loan - has personal loan?	
-contact - contact communication type (categorical: 'cellular','telephone')	
-day_of_week - last contact day of the week	
-month - last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')	
-duration - last contact duration, in seconds (numeric)
-day - date of the month
-campaign - number of contacts performed during this campaign and for this client (numeric, includes last contact)	
-pdays - number of days that passed by after the client was last contacted from a previous campaign (numeric; -1 means client was not previously contacted)	
-previous - number of contacts performed before this campaign and for this client	
-poutcome - outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')	
-y - has the client subscribed a term deposit?	

# Libraries
```{r, warning =FALSE, message=FALSE}
library(readr)
library(dplyr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(party)
library(caret)
library(caTools)
library(pROC)
```

# Reading in the data
```{r, message=FALSE,warning=FALSE}
bank.data = read_delim("bank.csv", delim = ";", 
    escape_double = FALSE, trim_ws = TRUE)
```

Inspecting the class of each variable
```{r}
sapply(bank.data, class)
```
Adjusting all binary variables into factors
```{r}
bank.data$default = as.factor(bank.data$default)
bank.data$housing = as.factor(bank.data$housing)
bank.data$loan = as.factor(bank.data$loan)
bank.data$y = as.factor(bank.data$y)
```

Checking for any NA values in the dataset
```{r}
any(is.na(bank.data)) # Finding if there are any NA values in our dataset
```

Upon inspection, there are unknown inputs across different columns in the dataset.

Some inputs are unknown. Therefore I will remove them since we have a relatively large number of observations
```{r}
adj.bank.data = subset(bank.data, contact!= "unknown")
adj.bank.data = subset(adj.bank.data, job!= "unknown")
adj.bank.data = subset(adj.bank.data, marital!= "unknown")
adj.bank.data = subset(adj.bank.data, education!= "unknown")
adj.bank.data = subset(adj.bank.data, loan!= "unknown")
adj.bank.data = subset(adj.bank.data, month!= "unknown")
```

A large number of the outcomes of the previous marketing campaign are unknown which isn't very useful in comparative analysis across levels.
```{r}
print(sum(bank.data$poutcome == "unknown")) # Finding the number of outcomes of previous marketing campaign which are unknown
```

Remove unnecessary columns
```{r}
adj.bank.data$poutcome = NULL
adj.bank.data$day = NULL # Removing date of the month
adj.bank.data$month = NULL
```

A large number of previous number of days that passed by after the client was last contacted from a previous campaign are -1, indicating most of the clients have not been contacted before. Therefore, I will remove pdays.
```{r}
print(sum(bank.data$pdays == -1))
```

```{r}
# Remove pdays column 
adj.bank.data$pdays = NULL
```

Is the dataset imbalanced?
```{r}
ggplot(data = adj.bank.data, mapping = aes(y=y))+
  geom_bar(fill="orange")+
  labs(x="Count", y="Result Type", title="Count of clients subscribing to a term deposit vs not subscribing")+
    theme(plot.title = element_text(hjust = 0.5))
```

Massively imbalanced dataset where there more negative case is the majority class, and the positive case is the minority class.

Upsampling the minority class (yes to subscribing) to create a balanced dataset
```{r}
balanced_data = upSample(x = adj.bank.data[, -which(names(adj.bank.data) == "y")], 
                          y = adj.bank.data$y)
colnames(balanced_data)[colnames(balanced_data) == "Class"] = "y"
```

Double checking the dataset is balanced
```{r}
table(balanced_data$y)
```

Splitting the data into training and test sets
```{r}
# Splitting data in train and test data 
split = sample.split(balanced_data$y, SplitRatio = 0.7) # 70-30 split for training vs test set

train.data = subset(balanced_data, split == "TRUE") # Subsetting dataset into training set
test.data = subset(balanced_data, split == "FALSE") # Subsetting dataset into test set
test.y = test.data$y
test.data$y = NULL
```

Double checking the response is now balanced between positive and negative class.
```{r}
table(train.data$y)
```

Fitting a decision tree with all possible variables
```{r}
tree_model = rpart(y ~ ., data = train.data, method = "class")
```

```{r}
print(tree_model) # Inspecting the model
plot(tree_model) # Visualising the tree
```

Creating predictions and confusion matrix based on the decision tree
```{r}
predictions = predict(tree_model, newdata = test.data, type = "class") # Predictions

conf_matrix = confusionMatrix(predictions, test.y) # Building Confusion matrix
print(conf_matrix$byClass)
```
Not amazing predictive statistics from the decision tree, but we can do better.

How about a Random Forest method? Random Forest delivers highly accurate predictions even with large datasets, effectively handles missing data without sacrificing accuracy, eliminates the need for normalization or standardisation, and reduces the risk of overfitting by combining multiple decision trees.

```{r}
rf.model = randomForest(y~., data = train.data, importance = TRUE)
```

Inspecting the random forest model
```{r}
print(rf.model)
```
Out-of-Bag (OOB) error estimate: 5.79%. This suggests that the model performs well, as the error rate is relatively low.

Looking at the error rates
```{r}
plot(rf.model)
```
The black line in the plot represents the overall OOB (Out-of-Bag) error rate, while the red dashed line corresponds to the error rate for the negative class The green dashed line indicates the error rate for the positive class As the number of trees increases, the error rates stabilise, showing that the random forest has enough trees to generalise effectively. The overall OOB error, represented by the black line, stabilises around 5%, which aligns with the previously observed result of 5.41%. For the class-specific error rates, the error for the "no" class, shown by the red dashed line, is higher and stabilizes around 10%, reflecting the higher class error seen in the confusion matrix. On the other hand, the error for the "yes" class, depicted by the green dashed line, is significantly lower, stabilising near 0.6%, which is consistent with its strong predictive performance.

Checking the important features
```{r}
# Feature importance
importance(rf.model)
varImpPlot(rf.model)
```
duration is the most critical variable, as excluding it would lead to the largest drop in accuracy, followed by age and balance (average of the two plots). Similarly, duration is the most important variable when it comes to contributing to the homogeneity of the nodes and leaves in the decision trees.

The bottom 3 variables in both plots: default, contact, and loan are the least important when it comes to a drop in accuracy and the Gini.

Therefore, I'll refit the model and omit default, contact, and loan.
```{r}
new.train = train.data
# new training data without 'default', 'contact', and 'loan'.
new.train$default = NULL
new.train$contact = NULL
new.train$loan = NULL

new.test = test.data  
# new test data without 'default', 'contact', and 'loan'.
new.test$default = NULL
new.test$contact = NULL
new.test$loan = NULL
```

```{r}
# adjusted Random Forest model without default, contact, and loan
adj.rf.model = randomForest(y~., data = new.train, importance=TRUE)
```

Inspecting the adjusted random forest model
```{r}
# Evaluate the model
print(adj.rf.model)
```
OOB estimate of  error rate: 4.98% which is an improvement from the original model.

```{r}
plot(adj.rf.model)
```
Similar story to the original model rf.model.

Let's inspect the relationship between the most important variables and the probability of a yes to subscribing to the term deposits.

Starting with duration
```{r}
# Inspecting the relationship of the last contact duration
partialPlot(adj.rf.model, new.train, x.var = "duration")
```
As the length of the last contact duration increases, the probability of a subscription to the term deposit decreases. Key message: keep marketing short and sweet to minimise a 'no' to the term deposit subscription.

How about age?
```{r}
partialPlot(adj.rf.model, new.train, x.var = "age")
```

Upwards overall trend until just before 45, then there is a downwards trend up until 50. Moreover, from 50 to 60, there is an overall upwards trend with some dips in between. Just before age 60, there is a sharp downwards dip until just after age 60. Finally, there is a gradual rise post-60, then some indication of convergence. Overall, from ages 20 to 45, there is the highest probability of attaining a 'yes' while post-60 onwards is the lowest probability.

What about balance - average yearly balance?
```{r}
# Inspecting the relationship of the last contact duration
partialPlot(adj.rf.model, new.train, x.var = "balance")
```

Looking from 0 onwards, there is an increasing trend as balance increases. In other words, the probability of a 'yes' to a subscription towards term deposits increases as balance increases.

Creating predictions and constructing the confusion matrix for the Random Forest model
```{r}
# Creating predictions based on the fitted model and test data
predictions.rf = predict(adj.rf.model, newdata = new.test, type = "class") 

conf_matrix.rf = confusionMatrix(predictions.rf, test.y) # Creating the confusion matrix for the Random Forest model
print(conf_matrix.rf$byClass)
```
Good statistics.

```{r, warning=FALSE}
# Estimate of AUC 
roc_curve = roc(test.y, as.numeric(predictions.rf)) # ROC curve
plot(roc_curve, grid=TRUE, col="orange",print.thres = "best") #Plot ROC curve alongside the point that maximises both sensitivity and specificity
```

What's the AUC for the ROC curve?
```{r}
print(auc(roc_curve))
```
The adjusted Random Forest model (adj.rf.model) is a great predictor of the binary classes. The model is good at distinguishing between a 'yes' or a 'no' in the response.

What about the error rate of the Random Forest model?
```{r}
adj.rf.model$err.rate
```
As more trees are added to the model, the OOB error decreases until around 0.05.

# Final model
```{r}
# Final model
print(adj.rf.model)
```


# Actionable insight
The most important features of the marketing campaign is duration - last contact duration, in seconds, age, and balance - average yearly balance.

As per the PDP, the probability of a yes to the subscription to the term deposit decreases as the last contact duration increases. Continuing on the topic of PDP, with regard to ages; from ages 20 to 45, there is the highest probability of attaining a 'yes' while post-60 onwards is the lowest probability. There is an increasing trend as balance increases. In other words, the probability of a 'yes' to a subscription towards term deposits increases as balance increases. 

Therefore there are 3 key takeaways from the PDP plots:
 - Keep the duration of the phone call short and sweet to promote likelihood of a 'yes'
 - Target ages groups 20 to 45 years old, and minimise efforts to post-60 year old clients
 - Call clients with sufficient and stable balances; Established clients should be prioritised.

Saving the model into a file for access
```{r}
saveRDS(adj.rf.model, "bankmarketing_rf.rds")
```